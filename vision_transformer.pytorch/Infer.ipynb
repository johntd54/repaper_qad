{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "amp_0.pth: 0.04988\n",
    "amp_1.pth: 0.10548\n",
    "amp_2.pth: 0.14452\n",
    "amp_3.pth: 0.18068\n",
    "amp_4.pth: 0.20834\n",
    "amp_5.pth: 0.23518\n",
    "amp_6.pth: 0.25766\n",
    "amp_7.pth: 0.27846\n",
    "amp_8.pth: 0.29652\n",
    "amp_9.pth: 0.3141\n",
    "amp_10: 32.882\n",
    "amp_11: 34.244\n",
    "amp_12: 35.138\n",
    "amp_13: 36.166\n",
    "amp_14: 37.16\n",
    "amp_15: 37.574\n",
    "amp_16.pth: 0.39168\n",
    "amp_17.pth: 0.39838\n",
    "amp_18.pth: 0.40566\n",
    "amp_19.pth: 0.40992\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from model import VisionTransformer\n",
    "from data import imagenet_train_transform, imagenet_eval_transform, CLASS_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model', 'optimizer'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('/home/john/john/repaper_qad/vision_transformer.pytorch/logs/amp_12.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embedding): Conv2d(3, 768, kernel_size=(28, 28), stride=(28, 28))\n",
       "  (encoder): Encoder(\n",
       "    (blocks): Sequential(\n",
       "      (0): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (key_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (query_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (value_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (final_project): Linear(in_features=9216, out_features=768, bias=True)\n",
       "          (drop_layer): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_layer1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_layer2): Dropout(p=0.1, inplace=False)\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (key_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (query_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (value_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (final_project): Linear(in_features=9216, out_features=768, bias=True)\n",
       "          (drop_layer): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_layer1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_layer2): Dropout(p=0.1, inplace=False)\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (key_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (query_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (value_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (final_project): Linear(in_features=9216, out_features=768, bias=True)\n",
       "          (drop_layer): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_layer1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_layer2): Dropout(p=0.1, inplace=False)\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (key_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (query_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (value_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (final_project): Linear(in_features=9216, out_features=768, bias=True)\n",
       "          (drop_layer): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_layer1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_layer2): Dropout(p=0.1, inplace=False)\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (key_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (query_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (value_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (final_project): Linear(in_features=9216, out_features=768, bias=True)\n",
       "          (drop_layer): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_layer1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_layer2): Dropout(p=0.1, inplace=False)\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (key_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (query_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (value_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (final_project): Linear(in_features=9216, out_features=768, bias=True)\n",
       "          (drop_layer): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_layer1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_layer2): Dropout(p=0.1, inplace=False)\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (key_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (query_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (value_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (final_project): Linear(in_features=9216, out_features=768, bias=True)\n",
       "          (drop_layer): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_layer1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_layer2): Dropout(p=0.1, inplace=False)\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (key_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (query_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (value_project): Linear(in_features=768, out_features=9216, bias=True)\n",
       "          (final_project): Linear(in_features=9216, out_features=768, bias=True)\n",
       "          (drop_layer): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (drop_layer1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_layer2): Dropout(p=0.1, inplace=False)\n",
       "        (linear): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=3072, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VisionTransformer()\n",
    "model.load_state_dict(state_dict['model'])\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = datasets.ImageFolder(\n",
    "    root='/home/john/john/data/imagenet/val',\n",
    "    transform=imagenet_eval_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.ImageFolder(\n",
    "    root='/home/john/john/data/imagenet/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: 0, tench, Tinca tinca\n",
      "Label: 0, tench, Tinca tinca\n"
     ]
    }
   ],
   "source": [
    "IDX = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_, label_ = val[IDX]\n",
    "    input_ = input_.cuda().unsqueeze(0)\n",
    "    output = model(input_)\n",
    "    output = torch.softmax(output, dim=1)\n",
    "    pred = torch.argmax(output, dim=1).item()\n",
    "    print(f\"Pred: {pred}, {CLASS_NAME[pred]}\")\n",
    "    print(f\"Label: {label_}, {CLASS_NAME[label_]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val, batch_size=256, num_workers=16).__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35138\n"
     ]
    }
   ],
   "source": [
    "val_loader = DataLoader(val, batch_size=256, num_workers=16)\n",
    "model.eval()\n",
    "total, correct = 0, 0\n",
    "idx = 0\n",
    "for input_, label_ in val_loader:\n",
    "    with torch.no_grad():\n",
    "        input_, label_ = input_.cuda(), label_.cuda()\n",
    "        output = model(input_)\n",
    "        output = torch.softmax(output, dim=1)\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        \n",
    "        total += output.size(0)\n",
    "        correct += (label_ == pred).sum().item()\n",
    "    idx += 1\n",
    "\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for idx in range(16,20):\n",
    "    file_name = f'amp_{idx}.pth'\n",
    "    state_dict = torch.load(f'/home/john/john/repaper_qad/vision_transformer.pytorch/logs/{file_name}')\n",
    "    model = VisionTransformer()\n",
    "    model.load_state_dict(state_dict['model'])\n",
    "    model.cuda()\n",
    "\n",
    "    val_loader = DataLoader(val, batch_size=256, num_workers=16)\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    idx = 0\n",
    "    for input_, label_ in val_loader:\n",
    "        with torch.no_grad():\n",
    "            input_, label_ = input_.cuda(), label_.cuda()\n",
    "            output = model(input_)\n",
    "            output = torch.softmax(output, dim=1)\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "\n",
    "            total += output.size(0)\n",
    "            correct += (label_ == pred).sum().item()\n",
    "        idx += 1\n",
    "\n",
    "    result.append(f'{file_name}: {correct / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amp_16.pth: 0.39168',\n",
       " 'amp_17.pth: 0.39838',\n",
       " 'amp_18.pth: 0.40566',\n",
       " 'amp_19.pth: 0.40992']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/mnt/ai_filestore/home/john/miniconda3/envs/vision-transformer/lib/python3.7/site-packages/torchvision/datasets/imagenet.py\u001b[0m(217)\u001b[0;36mparse_val_archive\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    215 \u001b[0;31m    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    216 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 217 \u001b[0;31m    \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    218 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    219 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0mwnid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwnids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  pp val_root\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/john/john/data/val'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    }
   ],
   "source": [
    "datasets.imagenet.parse_val_archive(\n",
    "    root='/home/john/john/data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "train_dir = Path('/home/john/john/data/imagenet/train')\n",
    "val_dir = Path('/home/john/john/data/imagenet/val')\n",
    "\n",
    "train_classes = [each.name for each in train_dir.glob('*')]\n",
    "val_classes = [each.name for each in val_dir.glob('*')]\n",
    "\n",
    "print(len(train_classes), len(val_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(val_classes).difference(set(train_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision-transformer",
   "language": "python",
   "name": "vision-transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
